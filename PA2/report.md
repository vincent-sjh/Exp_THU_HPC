# 实验二：全源最短路 (APSP)

宋建昊 2022010853

## 实现方法概述
- 通过 GPU 并行化显著提高了性能，核心思想是将矩阵划分为块（`tiles`），并利用 GPU 的共享内存和并行线程处理这些块。
- 代码包含三个主要 GPU Kernel：
- `Kernel 1 (kernel_center)`：更新枢轴块（`pivot block`），即当前迭代中包含枢轴元素的矩阵块。使用单个块（`grid_center(1, 1)`），因为只处理枢轴块。
- `Kernel 2 (kernel_row_col)`：更新依赖枢轴块的行块和列块。用 `num_blocks x 2` 的网格，其中一个维度处理行块，另一个处理列块。
- `Kernel 3 (kernel_peripheral)`：更新依赖枢轴行和列块的剩余外围块。使用 `num_blocks x num_blocks` 的网格，处理所有外围块。
- 按顺序启动内核确保了依赖关系（`kernel_center ----> kernel_row_col ----> kernel_peripheral`）。


## 存储层次利用
- 共享内存：所有内核使用共享内存存储频繁访问的块（如枢轴块、行/列块），显著减少全局内存访问。每个线程块的共享内存大小为 `64x64x4` 字节（约 16KB），适合现代 GPU 的共享内存容量。
- 全局内存：仅用于初始数据加载和最终结果写回。合并内存访问确保高效利用全局内存带宽。
- 寄存器：在 `kernel_peripheral` 中，当前块的元素存储在寄存器中，减少共享内存使用。
- 常量内存：未显式使用，但 `UNREACHABLE` 等常量通过编译器优化可能存储在常量内存中。

## 优化方法和思路

分块（`Tiling`）：将矩阵划分为`64x64` 的块，适配共享内存大小，减少全局内存访问，每个块独立计算，适合 GPU 的并行处理。

共享内存利用：所有频繁访问的数据（如枢轴块、行/列块）存储在共享内存，显著降低全局内存访问延迟，例如`kernel_center` 的 `shared_tile` 缓存整个枢轴块。

合并内存访问：线程以连续方式访问全局内存（如 `matrix[global_row * dim + global_col]`），确保高效的内存带宽利用，共享内存的加载和存储也遵循连续访问模式。

循环展开：`#pragma unroll 64` 展开枢轴循环，减少循环控制开销，优化指令流水线。

线程复用：每个线程处理四个元素（如 `val_00`, `val_01`, `val_10`, `val_11`），减少线程数，提高计算密度。

## 正确性保证
- 同步机制：`__syncthreads()` 确保数据加载到共享内存后所有线程同步，避免数据竞争。
- 边界处理：`load_to_shared` 和 `write_to_global` 函数包含边界检查，处理非 MATRIX_SIZE 整数倍的矩阵大小，确保正确性。



## 运行效果

下面是在数据规模为（1000, 2500, 5000, 7500,10000）等几种情况下的性能对比（运行时间），也测试了朴素实现的性能并计算了加速比。

| n     | apsp(ms)   | apsp_ref(ms) | speed-up ratio |
| ----- | ---------- | ------------ | -------------- |
| 1000  | 0.971190   | 15.109340    | 15.557553      |
| 2500  | 10.370829  | 376.403298   | 36.294427      |
| 5000  | 73.862556  | 2960.093823  | 40.075703      |
| 7500  | 251.487502 | 10021.324509 | 39.988201      |
| 10000 | 591.853247 | 22624.213489 | 38.226053      |
